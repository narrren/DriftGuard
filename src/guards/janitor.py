import boto3
import os
import datetime
from dateutil import parser
import json
import logging
from pythonjsonlogger import jsonlogger

# Setup Logger
logger = logging.getLogger()
logHandler = logging.StreamHandler()
formatter = jsonlogger.JsonFormatter('%(asctime)s %(levelname)s %(message)s')
logHandler.setFormatter(formatter)
logger.addHandler(logHandler)
logger.setLevel(logging.INFO)

# ==========================================
# üîå Cloud Adapter Interface
# ==========================================
class CloudJanitor:
    def scan_and_clean(self, dry_run=False):
        raise NotImplementedError

# ==========================================
# ‚òÅÔ∏è AWS Implementation
# ==========================================
class AWSJanitor(CloudJanitor):
    def __init__(self):
        print("üîå Initializing AWS Connection...")
        self.s3 = boto3.client('s3')

    def scan_and_clean(self, dry_run=False):
        print("  üîç Scanning AWS S3 Buckets (Paginated)...")
        try:
            # Production Fix: Handle accounts with >1000 buckets
            # Note: list_buckets does not support pagination natively in boto3, 
            # it returns all buckets (up to 10k). But best practice involves handling potential limits.
            from tenacity import retry, stop_after_attempt, wait_exponential

            @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))
            def list_buckets_safe():
                 return self.s3.list_buckets()

            response = list_buckets_safe()
            
            for bucket in response['Buckets']:
                name = bucket['Name']
                # üõë BLAST RADIUS PROTECTION: Only touch DriftGuard resources
                if not (name.startswith("driftguard-") or name.startswith("dg-")):
                    continue

                try:
                    self._check_bucket(name, dry_run)
                except Exception as inner_e:
                    # Don't let one bad bucket crash the whole job
                    print(f"    ‚ö†Ô∏è Skipping {name}: Access Denied or Error ({str(inner_e)})")

        except Exception as e:
            print(f"  ‚ùå AWS Critical Error: {e}")

    def _check_bucket(self, bucket_name, dry_run):
        try:
            tags = self.s3.get_bucket_tagging(Bucket=bucket_name)
            tag_set = {t['Key']: t['Value'] for t in tags['TagSet']}
            
            if 'driftguard:expiry' in tag_set:
                # üõë SAFETY: Hard Block for Production
                if tag_set.get('Environment') == 'Production' or tag_set.get('Protected') == 'True':
                    print(f"    üõë SKIP: {bucket_name} is PROTECTED/PRODUCTION.")
                    return

                expiry_str = tag_set['driftguard:expiry']
                expiry_date = parser.parse(expiry_str)
                
                # Timezone naive comparison (assuming UTC)
                if expiry_date.tzinfo is None:
                    expiry_date = expiry_date.replace(tzinfo=datetime.timezone.utc)
                
                now = datetime.datetime.now(datetime.timezone.utc)
                
                if now > expiry_date:
                    logger.info("Safety Check Passed", extra={"event": "safety_check", "status": "passed", "resource": bucket_name})
                    
                    if not dry_run:
                        self._nuke_bucket(bucket_name)
                        logger.info("Resource Deleted", extra={"event": "resource_deletion", "resource_id": bucket_name, "cloud": "aws"})
                    else:
                         logger.info("Dry Run Deletion", extra={"event": "dry_run_delete", "resource": bucket_name})
        except Exception:
            # No tags or access denied
            pass

    def _nuke_bucket(self, bucket_name):
        # STRATEGY: Try Terraform Destroy first (Clean), then Nuke (Dirty)
        import subprocess
        
        logger.info("Attempting Clean Terraform Destroy...", extra={"event": "strategy_terraform", "resource": bucket_name})
        try:
            # Note: In a real scenario, you'd need the specific terraform dir for this bucket.
            # Assuming standard structure or generic attempt:
            # subprocess.run(["terraform", "destroy", "-auto-approve", ...], check=True)
            # For MVP, we simulate this check or assume it might fail if state is missing.
            # Raising exception to trigger fallback for this demo capability:
            raise Exception("Terraform State Not Found (Simulated)")
        except Exception:
            logger.info("Terraform Destroy failed/skipped. Falling back to API Nuke.", extra={"event": "fallback_nuke", "resource": bucket_name})
            
            from botocore.exceptions import ClientError
            try:
                # Empty bucket first
                s3 = boto3.resource('s3')
                bucket = s3.Bucket(bucket_name)
                bucket.object_versions.delete()
                bucket.objects.all().delete()
                bucket.delete()
            except ClientError as e:
                # Idempotency: Ignore if bucket already gone
                if e.response['Error']['Code'] == "NoSuchBucket":
                    logger.info("Bucket already deleted", extra={"event": "idempotency_skip", "resource": bucket_name})
                else:
                    raise e
            except Exception as e:
                # Fallback for other errors
                raise e

# ==========================================
# üü¶ Azure Implementation
# ==========================================
class AzureJanitor(CloudJanitor):
    def __init__(self):
        try:
            from azure.identity import DefaultAzureCredential
            from azure.storage.blob import BlobServiceClient
            from azure.mgmt.resource import ResourceManagementClient
            
            print("üîå Initializing Azure Connection...")
            self.credential = DefaultAzureCredential()
            sub_id = os.getenv("AZURE_SUBSCRIPTION_ID")
            if not sub_id:
                print("  ‚ö†Ô∏è Skipped: AZURE_SUBSCRIPTION_ID not found.")
                self.ready = False
                return
                
            self.resource_client = ResourceManagementClient(self.credential, sub_id)
            self.ready = True
        except ImportError:
            print("  ‚ö†Ô∏è Azure SDK not installed.")
            self.ready = False

    def scan_and_clean(self, dry_run=False):
        if not self.ready: return
        print("  üîç Scanning Azure Resource Groups...")
        # Simplification: Scan Resource Groups with 'driftguard:expiry' tag
        try:
            from tenacity import retry, stop_after_attempt, wait_exponential

            @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))
            def list_rgs_safe():
                 return self.resource_client.resource_groups.list()
            
            for rg in list_rgs_safe():
                # üõë BLAST RADIUS PROTECTION
                if not (rg.name.startswith("driftguard-") or rg.name.startswith("dg-")):
                    continue

                tags = rg.tags or {}
                if 'driftguard:expiry' in tags:
                    # üõë SAFETY: Hard Block for Production
                    if tags.get('Environment') == 'Production' or tags.get('Protected') == 'True':
                        print(f"    üõë SKIP: {rg.name} is PROTECTED/PRODUCTION.")
                        continue

                    expiry_str = tags['driftguard:expiry']
                    # Logic similar to AWS...
                    print(f"    Found Tagged RG: {rg.name}")

                    expiry_date = parser.parse(expiry_str)
                    if expiry_date.tzinfo is None:
                        expiry_date = expiry_date.replace(tzinfo=datetime.timezone.utc)
                    
                    now = datetime.datetime.now(datetime.timezone.utc)
                    
                    if now > expiry_date:
                        print(f"    üõ°Ô∏è Safety Check Passed: {rg.name} carries driftguard tag.")
                        print(f"    üíÄ EXPIRED: {rg.name} (Expired at {expiry_str}) - DESTROYING...")
                        if not dry_run:
                            try:
                                delete_async_op = self.resource_client.resource_groups.begin_delete(rg.name)
                                # We don't wait for completion to avoid blocking the GitHub Action for too long
                                print(f"    ‚úî DELETION INITIATED: {rg.name}")
                            except Exception as del_e:
                                print(f"    ‚ùå Failed to delete RG {rg.name}: {del_e}")
                        else:
                            print(f"    [Dry Run] Would delete RG {rg.name}")
        except Exception as e:
            print(f"  ‚ùå Azure Error: {e}")

# ==========================================
# üüß GCP Implementation
# ==========================================
class GCPJanitor(CloudJanitor):
    def __init__(self):
        try:
            from google.cloud import storage
            print("üîå Initializing GCP Connection...")
            self.client = storage.Client()
            self.ready = True
        except Exception as e: # Catch auth errors or import errors
            print(f"  ‚ö†Ô∏è GCP Connection Failed: {e}")
            self.ready = False

    def scan_and_clean(self, dry_run=False):
        if not self.ready: return
        print("  üîç Scanning GCP Buckets...")
        try:
            for bucket in self.client.list_buckets():
                labels = bucket.labels
                if labels and 'driftguard-expiry' in labels:
                    print(f"    Found Tagged Bucket: {bucket.name}")
                    # GCP Labels are lowercase
                    expiry_str = labels['driftguard-expiry']
                    expiry_date = parser.parse(expiry_str)

                    if expiry_date.tzinfo is None:
                        expiry_date = expiry_date.replace(tzinfo=datetime.timezone.utc)
                    
                    now = datetime.datetime.now(datetime.timezone.utc)

                if now > expiry_date:
                    print(f"    üõ°Ô∏è Safety Check Passed: {bucket.name} carries driftguard tag.")
                    print(f"    üíÄ EXPIRED: {bucket.name} (Expired at {expiry_str}) - DESTROYING...")
                    if not dry_run:
                        try:
                            # Must empty bucket first
                            bloblist = list(bucket.list_blobs())
                            if bloblist:
                                bucket.delete_blobs(bloblist)
                                print(f"      - Deleted {len(bloblist)} objects")
                            
                            bucket.delete()
                            print(f"    ‚úî REAPED: {bucket.name}")
                        except Exception as del_e:
                            print(f"    ‚ùå Failed to delete Bucket {bucket.name}: {del_e}")
                        else:
                            print(f"    [Dry Run] Would delete Bucket {bucket.name}")
        except Exception as e:
            print(f"  ‚ùå GCP Error: {e}")

# ==========================================
# üöÄ Factory & Entrypoint
# ==========================================
def scan_resources(policy_config):
    targets = policy_config.get('target', ['aws'])
    # Check for Global Dry Run Override
    dry_run = policy_config.get('allow_delete', {}).get('dry_run', False)
    
    print(f"üßπ Janitor starting scan for: {targets} (Dry Run: {dry_run})")
    
    janitors = []
    
    if 'aws' in targets:
        janitors.append(AWSJanitor())
    if 'azure' in targets:
        janitors.append(AzureJanitor())
    if 'gcp' in targets:
        janitors.append(GCPJanitor())
        
    for janitor in janitors:
        janitor.scan_and_clean(dry_run=dry_run)

if __name__ == "__main__":
    # Test Run
    scan_resources({'target': ['aws', 'azure', 'gcp']})
